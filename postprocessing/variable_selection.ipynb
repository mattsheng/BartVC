{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyarrow.feather as feather\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clustering_with_scaler(\n",
    "    df, cols, algos, name, scaler=None, log1p=True, low=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies clustering algorithms to each row of the DataFrame with optional scaling and transformation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data for clustering.\n",
    "    - cols: List of column names to use for clustering.\n",
    "    - algos: Dictionary of clustering algorithms to apply.\n",
    "    - scaler: Scaler object (e.g., StandardScaler) to apply to the data.\n",
    "    - log1p: Logical flag for log1p transformation before applying scaler.\n",
    "    - low: Logical flag to indicate whether to identify the cluster with the lowest mean (True) or highest mean (False).\n",
    "\n",
    "    Returns:\n",
    "    - results_df: DataFrame with clustering results and metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Prepare the data\n",
    "        data = np.column_stack([row[col] for col in cols])\n",
    "        true_labels = np.array([1] * row[\"p0\"] + [0] * (row[\"p\"] - row[\"p0\"]))\n",
    "\n",
    "        for algo_name, algorithm in algos.items():\n",
    "            try:\n",
    "                transformed_data = transform_data(data, scaler, log1p)\n",
    "\n",
    "                # Fit the clustering algorithm\n",
    "                if algo_name == \"GaussianMixture\":\n",
    "                    algorithm.fit(transformed_data)\n",
    "                    predicted_labels = algorithm.predict(transformed_data)\n",
    "                else:\n",
    "                    predicted_labels = algorithm.fit_predict(transformed_data)\n",
    "\n",
    "                # Calculate cluster means and store them in a dictionary\n",
    "                cluster_means = {\n",
    "                    i: np.mean(data[:, 0][predicted_labels == i])\n",
    "                    for i in np.unique(predicted_labels)\n",
    "                }\n",
    "\n",
    "                # Identify the cluster with low/high mean\n",
    "                if low:\n",
    "                    cluster_pos = min(cluster_means, key=cluster_means.get)\n",
    "                else:\n",
    "                    cluster_pos = max(cluster_means, key=cluster_means.get)\n",
    "\n",
    "                # Assign label 1 to the selected cluster and label 0 otherwise\n",
    "                predicted_labels = np.where(predicted_labels == cluster_pos, 1, 0)\n",
    "                pos_idx = np.where(predicted_labels == 1)[0]\n",
    "\n",
    "                # Evaluate the performance\n",
    "                metrics = evaluate_clustering(true_labels, predicted_labels)\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"dataset_name\": row[\"dataset_name\"],\n",
    "                        \"random_state\": row[\"random_state\"],\n",
    "                        \"SNR\": row[\"SNR\"],\n",
    "                        \"n\": row[\"n\"],\n",
    "                        \"p\": row[\"p\"],\n",
    "                        \"p0\": row[\"p0\"],\n",
    "                        \"pos_idx\": pos_idx,\n",
    "                        \"Algorithm\": name,\n",
    "                        \"Runtime\": row[\"time_time\"],\n",
    "                        **metrics,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch exceptions for algorithms that fail\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"dataset_name\": row[\"dataset_name\"],\n",
    "                        \"random_state\": row[\"random_state\"],\n",
    "                        \"SNR\": row[\"SNR\"],\n",
    "                        \"n\": row[\"n\"],\n",
    "                        \"p\": row[\"p\"],\n",
    "                        \"p0\": row[\"p0\"],\n",
    "                        \"pos_idx\": pos_idx,\n",
    "                        \"Algorithm\": name,\n",
    "                        \"Runtime\": row[\"time_time\"],\n",
    "                        \"Error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def evaluate_clustering(true_labels, predicted_labels):\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "    tpr = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) * 100 if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) * 100 if (fn + tp) > 0 else 0\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    return {\"TPR\": tpr, \"FPR\": fpr, \"FNR\": fnr, \"F1\": f1}\n",
    "\n",
    "\n",
    "def evaluate_clustering_v2(pos_idx, p0, p):\n",
    "    tp = sum(pos_idx < p0)\n",
    "    fp = sum(pos_idx >= p0)\n",
    "    fn = p0 - tp\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "    tpr = tp / p0 * 100\n",
    "    fpr = fp / (p - p0) * 100\n",
    "    fnr = fn / p0 * 100\n",
    "    return {\"TPR\": tpr, \"FPR\": fpr, \"FNR\": fnr, \"F1\": f1}\n",
    "\n",
    "\n",
    "def MPM(df, algo, col, one_row=True):\n",
    "    \"\"\"\n",
    "    Variable selection via the Median Probability Model (MPM) criterion.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data for clustering.\n",
    "    - algos: Name of the algorithm.\n",
    "    - col: Column name to use for clustering.\n",
    "\n",
    "    Returns:\n",
    "    - results_df: DataFrame with clustering results and metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Prepare the data\n",
    "        vip = row[col] if one_row else row[col][0, :]\n",
    "        pos_idx = np.where(vip >= 0.5)[0]\n",
    "\n",
    "        # Evaluate the performance\n",
    "        metrics = evaluate_clustering_v2(pos_idx=pos_idx, p0=row[\"p0\"], p=row[\"p\"])\n",
    "        results.append(\n",
    "            {\n",
    "                \"dataset_name\": row[\"dataset_name\"],\n",
    "                \"random_state\": row[\"random_state\"],\n",
    "                \"SNR\": row[\"SNR\"],\n",
    "                \"n\": row[\"n\"],\n",
    "                \"p\": row[\"p\"],\n",
    "                \"p0\": row[\"p0\"],\n",
    "                \"pos_idx\": pos_idx,\n",
    "                \"Algorithm\": algo,\n",
    "                \"Runtime\": row[\"time_time\"],\n",
    "                **metrics,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def transform_data(data, scaler=None, log1p=True):\n",
    "    \"\"\"\n",
    "    Applies a log1p transformation followed by optional scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D array-like data to transform.\n",
    "    - scaler: Scaler object (e.g., StandardScaler) to apply.\n",
    "    - log1p: Logical flag for log1p transformation before applying scaler\n",
    "\n",
    "    Returns:\n",
    "    - Transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Log1p transformation\n",
    "    transformed_data = np.log1p(data) if log1p else data\n",
    "\n",
    "    # Apply scaler if provided\n",
    "    if scaler is not None:\n",
    "        transformed_data = scaler.fit_transform(transformed_data)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "\n",
    "def add_summary_stats(df, columns, K=[20]):\n",
    "    # Iterate over the dictionary to create new columns with the computed summaries\n",
    "    new_cols = {}\n",
    "    for k in K:\n",
    "        # Define a dictionary with the summary statistic suffix and corresponding lambda function\n",
    "        funcs = {\n",
    "            \"mu\": lambda x: np.mean(x[:k, :], axis=0),\n",
    "            \"q25\": lambda x: np.quantile(x[:k, :], q=0.25, axis=0),\n",
    "            \"q75\": lambda x: np.quantile(x[:k, :], q=0.75, axis=0),\n",
    "        }\n",
    "        for col in columns:\n",
    "            for suffix, func in funcs.items():\n",
    "                new_col_name = f\"{col}_{suffix}_K{k}\"\n",
    "                new_cols[new_col_name] = df[col].apply(func)\n",
    "\n",
    "    new_df = pd.DataFrame(new_cols)\n",
    "    df = pd.concat([df, new_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "    \"Agglomerative (average linkage)\": AgglomerativeClustering(\n",
    "        n_clusters=2, linkage=\"average\"\n",
    "    ),\n",
    "}\n",
    "notebook_dir = Path().resolve()\n",
    "in_root_path = notebook_dir.parent / \"results/no_idx/\"\n",
    "out_root_path = notebook_dir.parent / \"results/with_idx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable selection for \n",
    "- BART VIP Rank \n",
    "- BART VC-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BART_VC_measure = []\n",
    "BART_VIP_rank = []\n",
    "\n",
    "for n in [500, 1000, 1500, 2000]:\n",
    "    df_BART = pd.read_feather(\n",
    "        os.path.join(\n",
    "            in_root_path,\n",
    "            f\"BART_VIP_Rank/feynman_BART_VIP_Rank_n{n}_preidx.feather\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert list of 1D arrays into 2D arrays\n",
    "    df_BART[\"vip\"] = df_BART[\"vip\"].apply(lambda x: np.vstack(x))\n",
    "    df_BART[\"vip_rank\"] = df_BART[\"vip_rank\"].apply(lambda x: np.vstack(x))\n",
    "    df_BART[\"vc\"] = df_BART[\"vc\"].apply(lambda x: np.vstack(x))\n",
    "    df_BART[\"vc_rank\"] = df_BART[\"vc_rank\"].apply(lambda x: np.vstack(x))\n",
    "\n",
    "    # L=1 replicate\n",
    "    df_BART[\"vc_mu_K1\"] = df_BART[\"vc\"].apply(lambda x: x[0, :])\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    cols = [\"vip_rank\", \"vc_rank\", \"vc\"]\n",
    "    K = [20, 10]\n",
    "    df_BART = add_summary_stats(df_BART, columns=cols, K=K)\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_BART.drop([\"vc\", \"vc_rank\", \"vip\", \"vip_rank\"], axis=1, inplace=True)\n",
    "\n",
    "    # BART VC-measure\n",
    "    BART_VC_measure.append(\n",
    "        apply_clustering_with_scaler(\n",
    "            df=df_BART,\n",
    "            cols=[\"vc_mu_K10\", \"vc_q25_K10\", \"vc_rank_mu_K10\", \"vc_rank_q75_K10\"],\n",
    "            algos=algos,\n",
    "            name=\"BART VC-measure\",\n",
    "            scaler=StandardScaler(),\n",
    "            log1p=True,\n",
    "            low=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # BART VIP Rank\n",
    "    BART_VIP_rank.append(\n",
    "        apply_clustering_with_scaler(\n",
    "            df=df_BART,\n",
    "            cols=[\"vip_rank_mu_K20\"],\n",
    "            algos=algos,\n",
    "            name=\"BART VIP Rank\",\n",
    "            scaler=None,\n",
    "            log1p=False,\n",
    "            low=True,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Concatenate results\n",
    "BART_VC_measure = pd.concat(BART_VC_measure, ignore_index=True)\n",
    "BART_VIP_rank = pd.concat(BART_VIP_rank, ignore_index=True)\n",
    "\n",
    "# Store variable selection results\n",
    "feather.write_feather(\n",
    "    BART_VC_measure,\n",
    "    os.path.join(\n",
    "        out_root_path,\n",
    "        \"results_BART_VC_measure.feather\",\n",
    "    ),\n",
    ")\n",
    "feather.write_feather(\n",
    "    BART_VIP_rank,\n",
    "    os.path.join(\n",
    "        out_root_path,\n",
    "        \"results_BART_VIP_Rank.feather\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable selection for\n",
    "- DART\n",
    "- DART VC-measure\n",
    "- DART VIP-measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "DART_MPM = []\n",
    "DART_VIP = []\n",
    "DART_VC = []\n",
    "\n",
    "for n in [500, 1000, 1500, 2000]:\n",
    "    df_DART = pd.read_feather(\n",
    "        os.path.join(\n",
    "            in_root_path,\n",
    "            f\"DART_VC-measure/feynman_DART_VC-measure_n{n}_preidx.feather\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Convert list of 1D arrays into 2D arrays\n",
    "    df_DART[\"vip\"] = df_DART[\"vip\"].apply(lambda x: np.vstack(x))\n",
    "    df_DART[\"vip_rank\"] = df_DART[\"vip_rank\"].apply(lambda x: np.vstack(x))\n",
    "    df_DART[\"vc\"] = df_DART[\"vc\"].apply(lambda x: np.vstack(x))\n",
    "    df_DART[\"vc_rank\"] = df_DART[\"vc_rank\"].apply(lambda x: np.vstack(x))\n",
    "    df_DART[\"s\"] = df_DART[\"s\"].apply(lambda x: np.vstack(x))\n",
    "\n",
    "    # L=1 replicate\n",
    "    df_DART[\"vc_mu_K1\"] = df_DART[\"vc\"].apply(lambda x: x[0, :])\n",
    "\n",
    "    # Calculate summary statistics\n",
    "    cols = [\"vip_rank\", \"vip\", \"vc_rank\", \"vc\"]\n",
    "    K = [20, 15, 10, 5, 4, 3, 2]\n",
    "    df_DART = add_summary_stats(df_DART, columns=cols, K=K)\n",
    "\n",
    "    # Variable selection based on MPM\n",
    "    DART_MPM.append(MPM(df_DART, algo=\"DART MPM\", col=\"s\", one_row=False))\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df_DART.drop([\"vc\", \"vc_rank\", \"vip\", \"vip_rank\", \"s\"], axis=1, inplace=True)\n",
    "\n",
    "    # DART VC-measure\n",
    "    for l in [20, 15, 10, 5, 4, 3, 2, 1]:\n",
    "        if l == 1:\n",
    "            cols = [\"vc_mu_K1\"]\n",
    "        else:\n",
    "            cols = [\n",
    "                f\"vc_mu_K{l}\",\n",
    "                f\"vc_q25_K{l}\",\n",
    "                f\"vc_rank_mu_K{l}\",\n",
    "                f\"vc_rank_q75_K{l}\",\n",
    "            ]\n",
    "\n",
    "        results_tmp = apply_clustering_with_scaler(\n",
    "            df=df_DART,\n",
    "            cols=cols,\n",
    "            algos=algos,\n",
    "            name=f\"DART VC-measure (L={l})\",\n",
    "            scaler=StandardScaler(),\n",
    "            log1p=True,\n",
    "            low=False,\n",
    "        )\n",
    "        results_tmp[\"Runtime\"] = results_tmp[\"Runtime\"] / 20 * l\n",
    "        DART_VC.append(results_tmp)\n",
    "\n",
    "    # DART VIP-measure\n",
    "    DART_VIP.append(\n",
    "        apply_clustering_with_scaler(\n",
    "            df=df_DART,\n",
    "            cols=[\"vip_mu_K10\", \"vip_q25_K10\", \"vip_rank_mu_K10\", \"vip_rank_q75_K10\"],\n",
    "            algos=algos,\n",
    "            name=\"DART VIP-measure\",\n",
    "            scaler=StandardScaler(),\n",
    "            log1p=True,\n",
    "            low=False,\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Concatenate results\n",
    "DART_MPM = pd.concat(DART_MPM, ignore_index=True)\n",
    "DART_MPM[\"Runtime\"] /= 20\n",
    "DART_VC = pd.concat(DART_VC, ignore_index=True)\n",
    "DART_VIP = pd.concat(DART_VIP, ignore_index=True)\n",
    "\n",
    "# Store variable selection results\n",
    "feather.write_feather(\n",
    "    DART_MPM,\n",
    "    os.path.join(\n",
    "        out_root_path,\n",
    "        \"results_DART_MPM.feather\",\n",
    "    ),\n",
    ")\n",
    "feather.write_feather(\n",
    "    DART_VIP,\n",
    "    os.path.join(\n",
    "        out_root_path,\n",
    "        \"results_DART_VIP-measure.feather\",\n",
    "    ),\n",
    ")\n",
    "feather.write_feather(\n",
    "    DART_VC,\n",
    "    os.path.join(\n",
    "        out_root_path,\n",
    "        \"results_DART_VC-measure.feather\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate variable selection performance for\n",
    "- BART VIP-Local\n",
    "- BART VIP-G.SE\n",
    "- BART VIP-G.Max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "perm_dict = {\n",
    "    \"idx_gse\": \"BART VIP-G.SE\",\n",
    "    \"idx_gmax\": \"BART VIP-G.Max\",\n",
    "    \"idx_local\": \"BART VIP-Local\",\n",
    "}\n",
    "\n",
    "df_BART_perm = []\n",
    "for n in [500, 1000, 1500, 2000]:\n",
    "    df_BART_perm.append(\n",
    "        pd.read_feather(\n",
    "            os.path.join(\n",
    "                in_root_path,\n",
    "                f\"BART_perm/feynman_BART_perm_n{n}_preidx.feather\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "df_BART_perm = pd.concat(df_BART_perm, ignore_index=True)\n",
    "\n",
    "BART_perm = []\n",
    "for index, row in df_BART_perm.iterrows():\n",
    "    for col, algo in perm_dict.items():\n",
    "        # Evaluate the performance\n",
    "        metrics = evaluate_clustering_v2(row[col], row[\"p0\"], row[\"p\"])\n",
    "        BART_perm.append(\n",
    "            {\n",
    "                \"dataset_name\": row[\"dataset_name\"],\n",
    "                \"random_state\": row[\"random_state\"],\n",
    "                \"SNR\": row[\"SNR\"],\n",
    "                \"n\": row[\"n\"],\n",
    "                \"p\": row[\"p\"],\n",
    "                \"p0\": row[\"p0\"],\n",
    "                \"pos_idx\": [int(i) for i in row[col]],\n",
    "                \"Algorithm\": algo,\n",
    "                \"Runtime\": row[\"time_time\"],\n",
    "                **metrics,\n",
    "            }\n",
    "        )\n",
    "BART_perm = pd.DataFrame(BART_perm)\n",
    "feather.write_feather(\n",
    "    BART_perm,\n",
    "    os.path.join(\n",
    "        out_root_path,\n",
    "        \"results_BART_perm.feather\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate variable selection performance for BART MI-Local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BART_MI = []\n",
    "for n in [500, 1000, 1500, 2000]:\n",
    "    df_BART_MI.append(\n",
    "        pd.read_feather(\n",
    "            os.path.join(\n",
    "                in_root_path,\n",
    "                f\"BART_MI/feynman_BART_MI_n{n}_preidx.feather\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "df_BART_MI = pd.concat(df_BART_MI, ignore_index=True)\n",
    "\n",
    "BART_MI = []\n",
    "for index, row in df_BART_MI.iterrows():\n",
    "    metrics = evaluate_clustering_v2(row[\"idx_mi\"], row[\"p0\"], row[\"p\"])\n",
    "    BART_MI.append(\n",
    "        {\n",
    "            \"dataset_name\": row[\"dataset_name\"],\n",
    "            \"random_state\": row[\"random_state\"],\n",
    "            \"SNR\": row[\"SNR\"],\n",
    "            \"n\": row[\"n\"],\n",
    "            \"p\": row[\"p\"],\n",
    "            \"p0\": row[\"p0\"],\n",
    "            \"pos_idx\": [int(i) for i in row[\"idx_mi\"]],\n",
    "            \"Algorithm\": \"BART MI-Local\",\n",
    "            \"Runtime\": row[\"time_time\"],\n",
    "            **metrics,\n",
    "        }\n",
    "    )\n",
    "BART_MI = pd.DataFrame(BART_MI)\n",
    "feather.write_feather(\n",
    "    BART_MI,\n",
    "    os.path.join(\n",
    "        out_root_path,\n",
    "        \"results_BART_MI.feather\",\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variable selection for ABC Bayesian forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ABC = []\n",
    "for n in [500, 1000, 1500, 2000]:\n",
    "    df_ABC.append(\n",
    "        pd.read_feather(\n",
    "            os.path.join(\n",
    "                in_root_path,\n",
    "                f\"ABC_Bayesian_forests/feynman_ABC_Bayesian_forests_n{n}_preidx.feather\",\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "df_ABC = pd.concat(df_ABC, ignore_index=True)\n",
    "results_ABC = MPM(df_ABC, algo=\"ABC Bayesian forests\", col=\"s\", one_row=True)\n",
    "feather.write_feather(\n",
    "    results_ABC,\n",
    "    os.path.join(out_root_path, \"results_ABC_Bayesian_forests.feather\"),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sympy_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
