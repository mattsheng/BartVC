{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import f1_score, confusion_matrix\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pyarrow.feather as feather\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_clustering_with_scaler(\n",
    "    df, cols, algos, name, scaler=None, log1p=True, low=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Applies clustering algorithms to each row of the DataFrame with optional scaling and transformation.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data for clustering.\n",
    "    - cols: List of column names to use for clustering.\n",
    "    - algos: Dictionary of clustering algorithms to apply.\n",
    "    - scaler: Scaler object (e.g., StandardScaler) to apply to the data.\n",
    "    - log1p: Logical flag for log1p transformation before applying scaler.\n",
    "    - low: Logical flag to indicate whether to identify the cluster with the lowest mean (True) or highest mean (False).\n",
    "\n",
    "    Returns:\n",
    "    - results_df: DataFrame with clustering results and metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Prepare the data\n",
    "        data = np.column_stack([row[col] for col in cols])\n",
    "        true_labels = np.array([1] * row[\"p0\"] + [0] * (row[\"p\"] - row[\"p0\"]))\n",
    "\n",
    "        for algo_name, algorithm in algos.items():\n",
    "            try:\n",
    "                transformed_data = transform_data(data, scaler, log1p)\n",
    "\n",
    "                # Fit the clustering algorithm\n",
    "                if algo_name == \"GaussianMixture\":\n",
    "                    algorithm.fit(transformed_data)\n",
    "                    predicted_labels = algorithm.predict(transformed_data)\n",
    "                else:\n",
    "                    predicted_labels = algorithm.fit_predict(transformed_data)\n",
    "\n",
    "                # Calculate cluster means and store them in a dictionary\n",
    "                cluster_means = {\n",
    "                    i: np.mean(data[:, 0][predicted_labels == i])\n",
    "                    for i in np.unique(predicted_labels)\n",
    "                }\n",
    "\n",
    "                # Identify the cluster with low/high mean\n",
    "                if low:\n",
    "                    cluster_pos = min(cluster_means, key=cluster_means.get)\n",
    "                else:\n",
    "                    cluster_pos = max(cluster_means, key=cluster_means.get)\n",
    "\n",
    "                # Assign label 1 to the selected cluster and label 0 otherwise\n",
    "                predicted_labels = np.where(predicted_labels == cluster_pos, 1, 0)\n",
    "                pos_idx = np.where(predicted_labels == 1)[0]\n",
    "\n",
    "                # Evaluate the performance\n",
    "                metrics = evaluate_clustering(true_labels, predicted_labels)\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"dataset_name\": row[\"dataset_name\"],\n",
    "                        \"random_state\": row[\"random_state\"],\n",
    "                        \"SNR\": row[\"SNR\"],\n",
    "                        \"n\": row[\"n\"],\n",
    "                        \"p\": row[\"p\"],\n",
    "                        \"p0\": row[\"p0\"],\n",
    "                        \"pos_idx\": pos_idx,\n",
    "                        \"Algorithm\": name,\n",
    "                        \"Runtime\": row[\"time_time\"],\n",
    "                        **metrics,\n",
    "                    }\n",
    "                )\n",
    "\n",
    "            except Exception as e:\n",
    "                # Catch exceptions for algorithms that fail\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"dataset_name\": row[\"dataset_name\"],\n",
    "                        \"random_state\": row[\"random_state\"],\n",
    "                        \"SNR\": row[\"SNR\"],\n",
    "                        \"n\": row[\"n\"],\n",
    "                        \"p\": row[\"p\"],\n",
    "                        \"p0\": row[\"p0\"],\n",
    "                        \"pos_idx\": pos_idx,\n",
    "                        \"Algorithm\": name,\n",
    "                        \"Runtime\": row[\"time_time\"],\n",
    "                        \"Error\": str(e),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def evaluate_clustering(true_labels, predicted_labels):\n",
    "    tn, fp, fn, tp = confusion_matrix(true_labels, predicted_labels).ravel()\n",
    "    tpr = tp / (tp + fn) * 100 if (tp + fn) > 0 else 0\n",
    "    fpr = fp / (fp + tn) * 100 if (fp + tn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) * 100 if (fn + tp) > 0 else 0\n",
    "    f1 = f1_score(true_labels, predicted_labels)\n",
    "    return {\"TPR\": tpr, \"FPR\": fpr, \"FNR\": fnr, \"F1\": f1}\n",
    "\n",
    "\n",
    "def evaluate_clustering_v2(pos_idx, p0, p):\n",
    "    tp = sum(pos_idx < p0)\n",
    "    fp = sum(pos_idx >= p0)\n",
    "    fn = p0 - tp\n",
    "    f1 = 2 * tp / (2 * tp + fp + fn)\n",
    "    tpr = tp / p0 * 100\n",
    "    fpr = fp / (p - p0) * 100\n",
    "    fnr = fn / p0 * 100\n",
    "    return {\"TPR\": tpr, \"FPR\": fpr, \"FNR\": fnr, \"F1\": f1}\n",
    "\n",
    "\n",
    "def MPM(df, algo, col, one_row=True):\n",
    "    \"\"\"\n",
    "    Variable selection via the Median Probability Model (MPM) criterion.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing the data for clustering.\n",
    "    - algos: Name of the algorithm.\n",
    "    - col: Column name to use for clustering.\n",
    "\n",
    "    Returns:\n",
    "    - results_df: DataFrame with clustering results and metrics.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "\n",
    "    for index, row in df.iterrows():\n",
    "        # Prepare the data\n",
    "        vip = row[col] if one_row else row[col][0, :]\n",
    "        pos_idx = np.where(vip >= 0.5)[0]\n",
    "\n",
    "        # Evaluate the performance\n",
    "        metrics = evaluate_clustering_v2(pos_idx=pos_idx, p0=row[\"p0\"], p=row[\"p\"])\n",
    "        results.append(\n",
    "            {\n",
    "                \"dataset_name\": row[\"dataset_name\"],\n",
    "                \"random_state\": row[\"random_state\"],\n",
    "                \"SNR\": row[\"SNR\"],\n",
    "                \"n\": row[\"n\"],\n",
    "                \"p\": row[\"p\"],\n",
    "                \"p0\": row[\"p0\"],\n",
    "                \"pos_idx\": pos_idx,\n",
    "                \"Algorithm\": algo,\n",
    "                \"Runtime\": row[\"time_time\"],\n",
    "                **metrics,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Create a DataFrame with the results\n",
    "    results_df = pd.DataFrame(results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def transform_data(data, scaler=None, log1p=True):\n",
    "    \"\"\"\n",
    "    Applies a log1p transformation followed by optional scaling.\n",
    "\n",
    "    Parameters:\n",
    "    - data: 2D array-like data to transform.\n",
    "    - scaler: Scaler object (e.g., StandardScaler) to apply.\n",
    "    - log1p: Logical flag for log1p transformation before applying scaler\n",
    "\n",
    "    Returns:\n",
    "    - Transformed data.\n",
    "    \"\"\"\n",
    "\n",
    "    # Log1p transformation\n",
    "    transformed_data = np.log1p(data) if log1p else data\n",
    "\n",
    "    # Apply scaler if provided\n",
    "    if scaler is not None:\n",
    "        transformed_data = scaler.fit_transform(transformed_data)\n",
    "\n",
    "    return transformed_data\n",
    "\n",
    "\n",
    "def add_summary_stats(df, columns, K=[20]):\n",
    "    # Iterate over the dictionary to create new columns with the computed summaries\n",
    "    new_cols = {}\n",
    "    for k in K:\n",
    "        # Define a dictionary with the summary statistic suffix and corresponding lambda function\n",
    "        funcs = {\n",
    "            \"mu\": lambda x: np.mean(x[:k, :], axis=0),\n",
    "            \"q25\": lambda x: np.quantile(x[:k, :], q=0.25, axis=0),\n",
    "            \"q75\": lambda x: np.quantile(x[:k, :], q=0.75, axis=0),\n",
    "        }\n",
    "        for col in columns:\n",
    "            for suffix, func in funcs.items():\n",
    "                new_col_name = f\"{col}_{suffix}_K{k}\"\n",
    "                new_cols[new_col_name] = df[col].apply(func)\n",
    "\n",
    "    new_df = pd.DataFrame(new_cols)\n",
    "    df = pd.concat([df, new_df], axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = {\n",
    "    \"Agglomerative (average linkage)\": AgglomerativeClustering(\n",
    "        n_clusters=2, linkage=\"average\"\n",
    "    ),\n",
    "}\n",
    "notebook_dir = Path().resolve()\n",
    "in_root_path = notebook_dir.parent / \"results/no_idx/\"\n",
    "out_root_path = notebook_dir.parent / \"results/with_idx/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BART VC-measure toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_BART = pd.read_feather(\n",
    "    os.path.join(\n",
    "        in_root_path,\n",
    "        f\"BART_VIP_Rank/feynman_BART_VIP_Rank_n1000_preidx.feather\",\n",
    "    )\n",
    ")\n",
    "df_BART = df_BART[\n",
    "    (df_BART[\"SNR\"] == 1)\n",
    "    & (df_BART[\"dataset_name\"] == \"feynman_II_11_17\")\n",
    "    & (df_BART[\"random_state\"] == 860)\n",
    "]\n",
    "\n",
    "# Convert list of 1D arrays into 2D arrays\n",
    "df_BART[\"vip\"] = df_BART[\"vip\"].apply(lambda x: np.vstack(x))\n",
    "df_BART[\"vip_rank\"] = df_BART[\"vip_rank\"].apply(lambda x: np.vstack(x))\n",
    "df_BART[\"vc\"] = df_BART[\"vc\"].apply(lambda x: np.vstack(x))\n",
    "df_BART[\"vc_rank\"] = df_BART[\"vc_rank\"].apply(lambda x: np.vstack(x))\n",
    "\n",
    "# Calculate summary statistics\n",
    "cols = [\"vip_rank\", \"vip\", \"vc_rank\", \"vc\"]\n",
    "K = [10]\n",
    "df_BART = add_summary_stats(df_BART, columns=cols, K=K)\n",
    "\n",
    "# Remove raw columns\n",
    "df_BART.drop([\"vc\", \"vc_rank\", \"vip\", \"vip_rank\", \"s\", \"s_rank\"], axis=1, inplace=True)\n",
    "\n",
    "# BART VC-measure\n",
    "BART_VC_measure = apply_clustering_with_scaler(\n",
    "    df=df_BART,\n",
    "    cols=[\"vc_mu_K10\", \"vc_q25_K10\", \"vc_rank_mu_K10\", \"vc_rank_q75_K10\"],\n",
    "    algos=algos,\n",
    "    name=\"BART VC-measure\",\n",
    "    scaler=StandardScaler(),\n",
    "    log1p=True,\n",
    "    low=False,\n",
    ")\n",
    "\n",
    "BART_VC_measure_result = pd.merge(df_BART, BART_VC_measure, how=\"inner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DART VC-measure and VIP-measure toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_DART = pd.read_feather(\n",
    "    os.path.join(\n",
    "        in_root_path,\n",
    "        f\"DART_VC-measure/feynman_DART_VC-measure_n1000_preidx.feather\",\n",
    "    )\n",
    ")\n",
    "df_DART = df_DART[\n",
    "    (df_DART[\"SNR\"] == 1)\n",
    "    & (df_DART[\"dataset_name\"] == \"feynman_II_11_17\")\n",
    "    & (df_DART[\"random_state\"] == 860)\n",
    "]\n",
    "\n",
    "# Convert list of 1D arrays into 2D arrays\n",
    "df_DART[\"vip\"] = df_DART[\"vip\"].apply(lambda x: np.vstack(x))\n",
    "df_DART[\"vip_rank\"] = df_DART[\"vip_rank\"].apply(lambda x: np.vstack(x))\n",
    "df_DART[\"vc\"] = df_DART[\"vc\"].apply(lambda x: np.vstack(x))\n",
    "df_DART[\"vc_rank\"] = df_DART[\"vc_rank\"].apply(lambda x: np.vstack(x))\n",
    "\n",
    "# Calculate summary statistics\n",
    "cols = [\"vip_rank\", \"vip\", \"vc_rank\", \"vc\"]\n",
    "K = [10]\n",
    "df_DART = add_summary_stats(df_DART, columns=cols, K=K)\n",
    "\n",
    "# Remove raw columns\n",
    "df_DART.drop([\"vc\", \"vc_rank\", \"vip\", \"vip_rank\", \"s\", \"s_rank\"], axis=1, inplace=True)\n",
    "\n",
    "# DART VC-measure\n",
    "DART_VC_measure = apply_clustering_with_scaler(\n",
    "    df=df_DART,\n",
    "    cols=[\"vc_mu_K10\", \"vc_q25_K10\", \"vc_rank_mu_K10\", \"vc_rank_q75_K10\"],\n",
    "    algos=algos,\n",
    "    name=\"DART VC-measure\",\n",
    "    scaler=StandardScaler(),\n",
    "    log1p=True,\n",
    "    low=False,\n",
    ")\n",
    "\n",
    "# DART VIP-measure\n",
    "DART_VIP_measure = apply_clustering_with_scaler(\n",
    "    df=df_DART,\n",
    "    cols=[\"vip_mu_K10\", \"vip_q25_K10\", \"vip_rank_mu_K10\", \"vip_rank_q75_K10\"],\n",
    "    algos=algos,\n",
    "    name=\"DART VIP-measure\",\n",
    "    scaler=StandardScaler(),\n",
    "    log1p=True,\n",
    "    low=False,\n",
    ")\n",
    "\n",
    "DART_VC_measure_result = pd.merge(df_DART, DART_VC_measure, how=\"inner\")\n",
    "DART_VIP_measure_result = pd.merge(df_DART, DART_VIP_measure, how=\"inner\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat(\n",
    "    [BART_VC_measure_result, DART_VC_measure_result, DART_VIP_measure_result],\n",
    "    ignore_index=True,\n",
    ")\n",
    "feather.write_feather(\n",
    "    results,\n",
    "    os.path.join(\n",
    "        out_root_path,\n",
    "        \"results_toy_example.feather\",\n",
    "    ),\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sympy_new",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
